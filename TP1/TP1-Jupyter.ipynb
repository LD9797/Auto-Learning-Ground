{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MC8843 - Aprendizaje Automático**\n",
    "# **Trabajo práctico 1: Árboles de decisión**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-requisitos para el notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T03:17:04.874874500Z",
     "start_time": "2024-03-14T03:16:58.560189400Z"
    },
    "id": "aI7oDlIsH7yw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas\n",
    "import numpy as np\n",
    "import random\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dHTWUKUH7yt"
   },
   "source": [
    "## **1. Implementación de la clasificación multi-clase con árboles de decisión (60 puntos)**\n",
    "\n",
    "A continuación, implemente el algoritmo de maximización de la esperanza (descrito en el material del curso), usando la definición y descripción de las siguientes funciones como base:\n",
    "\n",
    "1. El conjunto de datos disponible en https://www.kaggle.com/yashsawarn/wifi-stretgth-for-rooms corresponde a lecturas de 7 fuentes de señal Wi-Fi, los cuales pretender ser utilizados para determinar si el receptor de las lecturas se encuentra en la habitación 1, 2, 3 o 4. La Figura 1 muestra una muestra del conjunto de datos, donde se observa que usualmente las lecturas de la señal Wi-Fi son valores negativos.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <figure>\n",
    "    <img src=\"images/sample.png\" alt=\"sample\">\n",
    "    <figcaption>Figura 1: Muestra del conjunto de datos a utilizar.</figcaption>\n",
    "  </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T03:23:15.306770800Z",
     "start_time": "2024-03-13T03:23:15.293654200Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_dataset(csv_name='wifi_localization.txt'):\n",
    "    \"\"\"\n",
    "\n",
    "    :param csv_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_frame = pandas.read_table(csv_name, sep=r'\\s+', names=('A', 'B', 'C', 'D', 'E', 'F', 'G', 'ROOM'),\n",
    "                                   dtype={'A': np.int64, 'B': np.float64, 'C': np.float64, 'D': np.float64,\n",
    "                                          'E': np.float64, 'F': np.float64, 'G': np.float64, 'ROOM': np.float64})\n",
    "    # targets_torch = torch.tensor(data_frame['ROOM'].values)\n",
    "    dataset_torch = torch.tensor(data_frame.values)\n",
    "    return dataset_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-64., -56., -61.,  ..., -82., -81.,   1.],\n",
      "        [-68., -57., -61.,  ..., -85., -85.,   1.],\n",
      "        [-63., -60., -60.,  ..., -85., -84.,   1.],\n",
      "        ...,\n",
      "        [-62., -59., -46.,  ..., -87., -88.,   4.],\n",
      "        [-62., -58., -52.,  ..., -90., -85.,   4.],\n",
      "        [-59., -50., -45.,  ..., -88., -87.,   4.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "dataset_torch = read_dataset()\n",
    "\n",
    "print(dataset_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Para resolver el problema de discriminar en cual habitación se encuentra el receptor de la señal Wi-Fi, su equipo decidió construir un arbol de decisión (CART por sus siglas en inglés). Para ello utilizará el código provisto en el \"notebook\" de \"Jupyter\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) El código provisto define las clases \"CART\" y \"Node_CART\", las cuales permiten construir un CART binario. Cada nodo del árbol tiene atributos como el \"feature\", el umbral, y el coeficiente de \"gini\" (o la entropía) de la partición definida en tal nodo. Además define el atributo \"dominant_class\" para el nodo, el cual es el resultado de calcular la clase con mayor cantidad de apariciones en la partición que define al nodo. Finalmente el código incluye la funcionalidad para generar un archivo \"xml\" (el cual se puede abrir en cualquier navegador web), para representar fácilmente el árbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) (10 puntos)** Implemente el método \"calculate_gini(data_partition_torch, num_classes = 4)\", el cual calcule el coeficiente de gini para el conjunto de datos recibido en un tensor de \"pytorch\". Para ello utilice la definición indicada en el material del curso. Realice una implementación matricial (prescindiendo de estructuras de repetición al máximo). Comente la implementación, detallando cada función utilizada en la documentación externa.\n",
    "\n",
    "$E_{\\textrm{gini},\\rho}\\left(\\tau_{d}\\right)=1-\\sum_{k=1}^{K}a_{k}^{2}$\n",
    "\n",
    "1) Diseñe e implemente al menos 2 pruebas unitarias para esta función."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) (10 puntos)** Implemente el método \"calculate_gini(data_partition_torch, num_classes = 4)\", el cual calcule el coeficiente de gini para el conjunto de datos recibido en un tensor de \"pytorch\". Para ello utilice la definición indicada en el material del curso. Realice una implementación matricial (prescindiendo de estructuras de repetición al máximo). Comente la implementación, detallando cada función utilizada en la documentación externa.\n",
    "\n",
    "$E_{\\textrm{entropy},\\rho}\\left(\\tau_{d}\\right)=-\\sum_{k=1}^{K}p\\left[k\\right]\\log\\left(p\\left[k\\right]\\right)$\n",
    "\n",
    "1) Diseñe e implemente al menos 2 pruebas unitarias para esta función."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) (30 puntos) Implemente los métodos \"select_best_feature_and_thresh(data_torch, num_classes = 4)\" y \"create_with_children\", de la clase \"Node_CART\". Este método recibe como parámetros el conjunto de datos en un tensor tipo torch a analizar. El método debe probar de forma extensiva todos los posibles features y sus correspondientes umbrales en los datos recibidos, hasta dar con el menor coeficiente ponderado de gini (o la mínima entropía, dependiendo de la función de error a utilizar). **Utilice indexación lógica para evitar al máximo el uso de estructuras de repetición tipo** \"for\". Solamente puede usar estructuras de repetición para iterar por los \"features\" y posibles umbrales dentro del conjunto de datos. Recuerde que para evaluar una posible partición, es necesario calcular el coeficiente de gini ponderado sugerido para decidir el feature y umbral óptimos es:\n",
    "\n",
    "$\\overline{E}_{\\textrm{gini}}\\left(\\tau_{d},d\\right)=\\frac{n_{i}}{n}E_{\\textrm{gini}}\\left(D_{i}\\right)+\\frac{n_{d}}{n}E_{\\textrm{gini}}\\left(D_{d}\\right).$\n",
    "\n",
    "Con un ponderado similar para la entropía. Comente la implementación, detallando cada función utilizada en la documentación externa.\n",
    "\n",
    "1) Diseñe e implemente al menos 2 pruebas unitarias para esta función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeCart:\n",
    "\n",
    "    def __init__(self,\n",
    "                 gini_entropy_function=\"GINI\",\n",
    "                 num_classes=4, num_features=7, ref_cart=None, current_depth=0):\n",
    "        \"\"\"\n",
    "\n",
    "        :param num_classes:\n",
    "        :param ref_cart:\n",
    "        :param current_depth:\n",
    "        \"\"\"\n",
    "        self.ref_CART = ref_cart\n",
    "        self.threshold_value = 0  # Umbral\n",
    "        self.feature_num = 0\n",
    "        self.node_right = None\n",
    "        self.node_left = None\n",
    "        self.data_torch_partition = None  # Referencia a la partición del dato\n",
    "        self.gini = 0  # O Entropia. Funcion numerica a utilizar cuando se construya el arbol.\n",
    "        self.dominant_class = None  # Clase con mayor cantidad de observaciones en esa particion.\n",
    "        self.accuracy_dominant_class = None  # Tasa de aciertos de esa clase dominante\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = num_features\n",
    "        self.current_depth = current_depth  # Profundidad\n",
    "        self.leaf = False\n",
    "        self.gini_function = gini_entropy_function\n",
    "        self.hits = 0\n",
    "        self.fails = 0\n",
    "\n",
    "    def to_xml(self, current_str=\"\"):\n",
    "        \"\"\"\n",
    "        Recursive function to write the node content to an xml formatted string\n",
    "        param current_str : the xml content so far in the whole tree\n",
    "        return the string with the node content\n",
    "        \"\"\"\n",
    "        str_node = (f\"<node>\"\n",
    "                    f\"<thresh>{self.threshold_value}</thresh>\"\n",
    "                    f\"<feature>{self.feature_num}</feature>\"\n",
    "                    f\"<depth>{self.current_depth}</depth>\"\n",
    "                    f\"<gini>{self.gini}</gini>\")\n",
    "        if self.node_left:\n",
    "            str_left = self.node_left.to_xml(current_str)\n",
    "            str_node += str_left\n",
    "        if self.node_right:\n",
    "            str_right = self.node_right.to_xml(current_str)\n",
    "            str_node += str_right\n",
    "        if self.is_leaf():\n",
    "            str_node += (f\"<dominant_class>{self.dominant_class}</dominant_class>\"\n",
    "                         f\"<acc_dominant_class>{self.accuracy_dominant_class}</acc_dominant_class>\")\n",
    "        str_node += \"</node>\"\n",
    "        return str_node\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"\n",
    "        Checks whether the node is a leaf\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.leaf\n",
    "\n",
    "    def create_with_children(self, current_depth=0, list_selected_features=None, min_gini=0.000001, max_cart_depth=3,\n",
    "                             min_observations=2, glob_list_selected_features=None):\n",
    "        \"\"\"\n",
    "        Creates a node by selecting the best feature and threshold, and if needed, creating its children\n",
    "        param data_torch: dataset with the current partition to deal with in the node\n",
    "        param current_depth: depth counter for the node\n",
    "        param list_selected_features: list of selected features so far for the CART building process\n",
    "        param min_gini: hyperparameter selected by the user defining the minimum tolerated Gini coefficient for a node\n",
    "        return the list of selected features so far\n",
    "        \"\"\"\n",
    "\n",
    "        if list_selected_features is None:\n",
    "            list_selected_features = []\n",
    "\n",
    "        if glob_list_selected_features is None:\n",
    "            glob_list_selected_features = []\n",
    "\n",
    "        min_thresh, min_feature, min_gini_thresh = (\n",
    "            self.select_best_feature_and_thresh(data_torch=self.data_torch_partition,\n",
    "                                                list_features_selected=list_selected_features))\n",
    "\n",
    "        self.feature_num = min_feature\n",
    "        self.threshold_value = min_thresh\n",
    "        self.gini = min_gini_thresh\n",
    "        self.current_depth = current_depth\n",
    "        list_selected_features.append(self.feature_num)\n",
    "\n",
    "        if (min_gini_thresh <= min_gini or len(list_selected_features) == self.num_features or\n",
    "                current_depth == max_cart_depth or self.data_torch_partition.shape[0] <= min_observations):\n",
    "            # This is a leaf\n",
    "            self.leaf = True\n",
    "            length = self.data_torch_partition.shape[1] - 1\n",
    "            tag_values = self.data_torch_partition[:, length:length + 1].squeeze()\n",
    "            tags, counts = tag_values.unique(return_counts=True)\n",
    "            most_common_value = tags[counts.argmax()].item()\n",
    "            self.dominant_class = most_common_value\n",
    "            return list_selected_features\n",
    "\n",
    "        left_idx = self.data_torch_partition[:, self.feature_num] < self.threshold_value\n",
    "        right_idx = self.data_torch_partition[:, self.feature_num] >= self.threshold_value\n",
    "\n",
    "        dataset_partition_left = self.data_torch_partition[left_idx]\n",
    "        dataset_partition_right = self.data_torch_partition[right_idx]\n",
    "\n",
    "        left_child = NodeCart(current_depth=current_depth, gini_entropy_function=self.gini_function)\n",
    "        left_child.data_torch_partition = dataset_partition_left\n",
    "        left_child.ref_CART = self\n",
    "\n",
    "        right_child = NodeCart(current_depth=current_depth, gini_entropy_function=self.gini_function)\n",
    "        right_child.data_torch_partition = dataset_partition_right\n",
    "        right_child.gini_function = self.gini_function\n",
    "        right_child.ref_CART = self\n",
    "\n",
    "        current_depth += 1\n",
    "\n",
    "        self.node_left = left_child\n",
    "        self.node_right = right_child\n",
    "\n",
    "        unique_features_left = list_selected_features.copy()\n",
    "        unique_features_right = list_selected_features.copy()\n",
    "\n",
    "        left_selected = self.node_left.create_with_children(current_depth, unique_features_left,\n",
    "                                                            max_cart_depth=max_cart_depth,\n",
    "                                                            min_gini=min_gini,\n",
    "                                                            min_observations=min_observations)\n",
    "        right_selected = self.node_right.create_with_children(current_depth, unique_features_right,\n",
    "                                                              min_gini=min_gini,\n",
    "                                                              max_cart_depth=max_cart_depth,\n",
    "                                                              min_observations=min_observations)\n",
    "\n",
    "        glob_list_selected_features.extend(left_selected)\n",
    "        glob_list_selected_features.extend(right_selected)\n",
    "\n",
    "        # TODO eliminar duplicados\n",
    "\n",
    "        return glob_list_selected_features\n",
    "\n",
    "    def select_best_feature_and_thresh(self, data_torch, list_features_selected=None, num_classes=4):\n",
    "        \"\"\"\n",
    "        Selects the best feature and threshold that minimizes the Gini coefficient\n",
    "        param data_torch: dataset partition to analyze\n",
    "        param list_features_selected list of features selected so far, thus must be ignored\n",
    "        param num_classes: number of K classes to discriminate from\n",
    "        return min_thresh, min_feature, min_gini found for the dataset partition when\n",
    "        selecting the found feature and threshold\n",
    "        \"\"\"\n",
    "        def evaluate_feature(data, feature_num, gini_entropy_total_function):\n",
    "            root_node = NodeCart()\n",
    "            root_node.data_torch_partition = data\n",
    "            root_node.feature_num = feature_num\n",
    "            threshold_values = torch.unique(data[:, feature_num:feature_num + 1].squeeze())\n",
    "            value_gini = {}\n",
    "            for value in threshold_values:\n",
    "                root_node.threshold_value = value\n",
    "                left_idx = data[:, root_node.feature_num] < root_node.threshold_value\n",
    "                right_idx = data[:, root_node.feature_num] >= root_node.threshold_value\n",
    "                dataset_partition_left = data[left_idx]\n",
    "                dataset_partition_right = data[right_idx]\n",
    "                left_child = NodeCart()\n",
    "                left_child.data_torch_partition = dataset_partition_left\n",
    "                right_child = NodeCart()\n",
    "                right_child.data_torch_partition = dataset_partition_right\n",
    "                gini = gini_entropy_total_function(left_child, right_child)\n",
    "                value_gini[value] = gini\n",
    "            minimum_gini = min(value_gini, key=value_gini.get)\n",
    "            return {minimum_gini.item(): value_gini[minimum_gini].item()}\n",
    "\n",
    "        if list_features_selected is None:\n",
    "            list_features_selected = []\n",
    "        num_features = data_torch.shape[1] - 1\n",
    "        if len(list_features_selected) == num_features:\n",
    "            raise ValueError(\"All features have been selected\")\n",
    "        features_gini = {}\n",
    "        for feature in range(num_features):\n",
    "            if feature not in list_features_selected:\n",
    "                features_gini[feature] = evaluate_feature(data_torch, feature, self.calculate_total_gini_entropy)\n",
    "        min_key, min_inner_dict = min(features_gini.items(), key=lambda item: next(iter(item[1].values())))\n",
    "        result = features_gini[min_key]\n",
    "        min_feature = min_key\n",
    "        min_thresh = list(result.keys())[0]\n",
    "        min_gini = result[min_thresh]\n",
    "        return min_thresh, min_feature, min_gini\n",
    "\n",
    "    def calculate_gini(self, data_partition_torch, num_classes=4):\n",
    "        \"\"\"\n",
    "        Calculates the Gini coefficient for a given partition with the given number of classes\n",
    "        param data_partition_torch: current dataset partition as a tensor\n",
    "        param num_classes: K number of classes to discriminate from\n",
    "        returns the calculated Gini coefficient\n",
    "        \"\"\"\n",
    "        def calculate_gini_impurity(partition):\n",
    "            size = partition.shape[0]\n",
    "            if size == 0:  # To handle the division by zero\n",
    "                return torch.tensor(0)\n",
    "            length = partition.shape[1] - 1\n",
    "            _, counts = partition[:, length].unique(return_counts=True)\n",
    "            gini = 1 - torch.sum((counts / size) ** 2)\n",
    "            return gini\n",
    "        return calculate_gini_impurity(data_partition_torch)\n",
    "\n",
    "    def calculate_entropy(self, data_partition_torch, num_classes=4):\n",
    "        \"\"\"\n",
    "        Calculates the Gini coefficient for a given partition with the given number of classes\n",
    "        param data_partition_torch: current dataset partition as a tensor\n",
    "        param num_classes: K number of classes to discriminate from\n",
    "        returns the calculated Gini coefficient\n",
    "        \"\"\"\n",
    "        def calculate_entropy_disorder(partition):\n",
    "            size = partition.shape[0]\n",
    "            if size == 0:  # To handle the division by zero\n",
    "                return torch.tensor(0)\n",
    "            length = partition.shape[1] - 1\n",
    "            epsilon = torch.tensor(0.0001)  # Small epsilon to prevent probabilities equal to 0\n",
    "            _, counts = partition[:, length].unique(return_counts=True)\n",
    "            probabilities = (counts / size) + epsilon\n",
    "            entropy = - torch.sum(probabilities * torch.log(probabilities))\n",
    "            return entropy\n",
    "        return calculate_entropy_disorder(data_partition_torch)\n",
    "\n",
    "    def calculate_total_gini_entropy(self, node_left, node_right):\n",
    "        selected_function = self.calculate_gini if self.gini_function == \"GINI\" else self.calculate_entropy\n",
    "        size_left = node_left.data_torch_partition.shape[0]\n",
    "        size_right = node_right.data_torch_partition.shape[0]\n",
    "        size_total = size_left + size_right\n",
    "        gini_entropy_left = selected_function(node_left.data_torch_partition)\n",
    "        gini_entropy_right = selected_function(node_right.data_torch_partition)\n",
    "        gini_entropy_total = (size_left / size_total) * gini_entropy_left + (size_right / size_total) * gini_entropy_right\n",
    "        return gini_entropy_total\n",
    "\n",
    "    def evaluate_node(self, input_torch):\n",
    "        \"\"\"\n",
    "        Evaluates an input observation within the node.\n",
    "        If is not a leaf node, send it to the corresponding node\n",
    "        return predicted label\n",
    "        \"\"\"\n",
    "        feature_val_input = input_torch[self.feature_num]\n",
    "        if self.is_leaf():\n",
    "            return self.dominant_class, self\n",
    "        elif feature_val_input < self.threshold_value:\n",
    "            return self.node_left.evaluate_node(input_torch)\n",
    "        else:\n",
    "            return self.node_right.evaluate_node(input_torch)\n",
    "\n",
    "    def update_accuracy(self):\n",
    "        self.accuracy_dominant_class = (self.hits / (self.hits + self.fails)) * 100\n",
    "        self.accuracy_dominant_class = round(self.accuracy_dominant_class, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CART:  # Este es el arbol\n",
    "    # Do not change default values or unit tests will be affected !!\n",
    "    def __init__(self, dataset_torch, max_cart_depth=3, min_observations=2, gini_entropy_function=\"GINI\", num_classes=4):\n",
    "        \"\"\"\n",
    "        CART has only one root node\n",
    "        \"\"\"\n",
    "        # min observations per node\n",
    "        self.min_observations = min_observations\n",
    "        self.root = NodeCart(num_classes=num_classes, ref_cart=self, gini_entropy_function=gini_entropy_function)\n",
    "        self.root.data_torch_partition = dataset_torch\n",
    "        self.max_CART_depth = max_cart_depth\n",
    "        self.list_selected_features = []\n",
    "        self.confusion_matrix = torch.zeros(num_classes, num_classes)\n",
    "\n",
    "    def get_root(self):\n",
    "        \"\"\"\n",
    "        Gets tree root\n",
    "        \"\"\"\n",
    "        return self.root\n",
    "\n",
    "    def get_min_observations(self):\n",
    "        \"\"\"\n",
    "        return min observations per node\n",
    "        \"\"\"\n",
    "        return self.min_observations\n",
    "\n",
    "    def get_max_depth(self):\n",
    "        \"\"\"\n",
    "        Gets the selected max depth of the tree\n",
    "        \"\"\"\n",
    "        return self.max_CART_depth\n",
    "\n",
    "    def build_cart(self):\n",
    "        \"\"\"\n",
    "        Build CART from root\n",
    "        \"\"\"\n",
    "        self.list_selected_features = self.root.create_with_children(max_cart_depth=self.max_CART_depth,\n",
    "                                                                     min_observations=self.min_observations)\n",
    "\n",
    "    def to_xml(self, xml_file_name):\n",
    "        \"\"\"\n",
    "        write Xml file with tree content\n",
    "        \"\"\"\n",
    "        str_nodes = self.root.to_xml()\n",
    "        with open(xml_file_name, 'w') as file:\n",
    "            file.write(str_nodes)\n",
    "        return str_nodes\n",
    "\n",
    "    def evaluate_input(self, input_torch):\n",
    "        \"\"\"\n",
    "        Evaluate a specific input in the tree and get the predicted class\n",
    "        \"\"\"\n",
    "        return self.root.evaluate_node(input_torch)\n",
    "\n",
    "    def update_confusion_matrix(self, estimated_class, real_class):\n",
    "        self.confusion_matrix[int(estimated_class) - 1][int(real_class) - 1] += 1\n",
    "\n",
    "    def get_f1_scores_per_class(self):\n",
    "        def get_metrics(matrix, the_class):\n",
    "            tp = matrix[the_class, the_class]\n",
    "            fn = torch.sum(matrix[:, the_class]) - tp\n",
    "            fp = torch.sum(matrix[the_class, :]) - tp\n",
    "            tn = torch.sum(matrix) - tp - fp - fn\n",
    "            return tp, tn, fp, fn\n",
    "        f1_scores = {}\n",
    "        for my_class in range(self.confusion_matrix.size(0)):\n",
    "            vp, vn, fp, fn = get_metrics(self.confusion_matrix, my_class)\n",
    "            sensibility_tvp = torch.nan_to_num(vp / (fn + vp))\n",
    "            accuracy_vpp = torch.nan_to_num(vp / (vp + fp))\n",
    "            f1_score = torch.nan_to_num((2 * sensibility_tvp * accuracy_vpp) / (sensibility_tvp + accuracy_vpp))\n",
    "            f1_scores[my_class + 1] = f1_score\n",
    "        return f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) (10 puntos) Implemente la función \"test_CART\" la cual evalúe un CART previamente entrenado para un conjunto de datos $D$ representado en un tensor. Calcule la tasa de aciertos (\"accuracy\"), definida como:\n",
    "\n",
    "$a=\\frac{c}{n}$\n",
    "\n",
    "donde $c$ corresponde a las estimaciones correctas, para tal conjunto de datos y retornela. Comente la implementación, detallando cada función utilizada en la documentación externa.\n",
    "\n",
    "1. Diseñe e implemente al menos 2 pruebas unitarias para esta función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cart(dataset_torch, name_xml=\"\", max_cart_depth=3, min_obs_per_leaf=2, gini_entropy_function=\"GINI\"):\n",
    "    \"\"\"\n",
    "    Train CART model\n",
    "    \"\"\"\n",
    "    tree = CART(dataset_torch=dataset_torch, max_cart_depth=max_cart_depth, min_observations=min_obs_per_leaf,\n",
    "                gini_entropy_function=gini_entropy_function)\n",
    "    tree.build_cart()\n",
    "    if name_xml:\n",
    "        tree.to_xml(name_xml)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cart(tree, testset_torch):\n",
    "    \"\"\"\n",
    "    Test a previously built CART\n",
    "    \"\"\"\n",
    "    # Entropy 14 vs 3 | 1490 vs 510\n",
    "    # Gini 11 vs 6 | 1397 vs 603\n",
    "    hits = 0\n",
    "    fails = 0\n",
    "    for observation in testset_torch:\n",
    "        expected = observation[-1]\n",
    "        result, leaf = tree.evaluate_input(observation)\n",
    "        tree.update_confusion_matrix(result, expected)\n",
    "        if expected == result:\n",
    "            leaf.hits += 1\n",
    "            leaf.update_accuracy()\n",
    "            hits += 1\n",
    "        else:\n",
    "            leaf.fails += 1\n",
    "            leaf.update_accuracy()\n",
    "            fails += 1\n",
    "    accuracy = (hits / (hits + fails)) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree accuracy is:  95.25\n"
     ]
    }
   ],
   "source": [
    "tree = train_cart(dataset_torch, name_xml=\"testCART.xml\", max_cart_depth=3,min_obs_per_leaf=2,gini_entropy_function=\"GINI\")\n",
    "\n",
    "print(\"Tree accuracy is: \", test_cart(tree=tree, testset_torch=dataset_torch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Evaluación del CART (40 puntos)**\n",
    "\n",
    "1. (20 puntos) Evalúe el CART implementado con el conjunto de datos completo provisto  https://www.kaggle.com/yashsawarn/wifi-stretgth-for-rooms usándolo como conjunto de datos de entrenamiento y prueba. Reporte la tasa de aciertos y el F1-score promedio de todas las clases, obtenida e incluya el código de la evaluación. Pruebe con una profundidad máxima de 3 y 4 nodos, siempre con mínimo 2 observaciones por hoja.\n",
    "\n",
    "    a) Realice lo anterior usando la entropía y el coeficiente de Gini. Compare y comente los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tree_full_dataset(max_cart_depth, gini_entropy_function, min_obs_per_leaf=2):\n",
    "    dataset = read_dataset(csv_name=\"wifi_localization.txt\")\n",
    "    tree = train_cart(dataset, max_cart_depth=max_cart_depth, gini_entropy_function=gini_entropy_function,\n",
    "                      min_obs_per_leaf=min_obs_per_leaf)\n",
    "    overall_accuracy = test_cart(tree, dataset)\n",
    "    f1_scores = tree.get_f1_scores_per_class()\n",
    "    print(f\"Overall accuracy: {overall_accuracy}\")\n",
    "    print(f\"F1 scores: {f1_scores}\")\n",
    "    return overall_accuracy, f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 95.25\n",
      "F1 scores: {1: tensor(0.9940), 2: tensor(0.9198), 3: tensor(0.9070), 4: tensor(0.9889)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(95.25,\n",
       " {1: tensor(0.9940), 2: tensor(0.9198), 3: tensor(0.9070), 4: tensor(0.9889)})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_tree_full_dataset(3, \"GINI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 96.55\n",
      "F1 scores: {1: tensor(0.9950), 2: tensor(0.9435), 3: tensor(0.9333), 4: tensor(0.9900)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(96.55,\n",
       " {1: tensor(0.9950), 2: tensor(0.9435), 3: tensor(0.9333), 4: tensor(0.9900)})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_tree_full_dataset(4, \"GINI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 94.85\n",
      "F1 scores: {1: tensor(0.9940), 2: tensor(0.9140), 3: tensor(0.8961), 4: tensor(0.9890)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(94.85,\n",
       " {1: tensor(0.9940), 2: tensor(0.9140), 3: tensor(0.8961), 4: tensor(0.9890)})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_tree_full_dataset(3, \"ENTROPY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 95.89999999999999\n",
      "F1 scores: {1: tensor(0.9940), 2: tensor(0.9248), 3: tensor(0.9247), 4: tensor(0.9920)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(95.89999999999999,\n",
       " {1: tensor(0.9940), 2: tensor(0.9248), 3: tensor(0.9247), 4: tensor(0.9920)})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_tree_full_dataset(4, \"ENTROPY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (20 puntos) Para una profundidad máxima de 3 y 4 nodos: evalúe el CART implementado usando 10 particiones aleatorias del conjunto de datos, con un 70% del conjunto de datos como conjunto de datos de entrenamiento, y el restante 30% como conjunto de datos de prueba. Reporte una tabla con la tasa de aciertos y F1-score promedio de todas las clases, de cada una de las 10 corridas, y el promedio y desviación estándar para las 10 corridas.\n",
    "\n",
    "    a) Realice lo anterior usando la entropía y el coeficiente de Gini. Compare los resultados y comente las posibles ventajas y desventajas de cada función de error. En la comparativa, incluya el tiempo de ejecución en entrenar cada modelo con cada función de error distinta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(csv_name='wifi_localization.txt'):\n",
    "    data_frame = pandas.read_table(csv_name, sep=r'\\s+', names=('A', 'B', 'C', 'D', 'E', 'F', 'G', 'ROOM'),\n",
    "                                   dtype={'A': np.int64, 'B': np.float64, 'C': np.float64, 'D': np.float64,\n",
    "                                          'E': np.float64, 'F': np.float64, 'G': np.float64, 'ROOM': np.float64})\n",
    "    shuffled_values = data_frame.sample(frac=1).reset_index(drop=True).values\n",
    "    split_index = int(len(shuffled_values) * 0.7)\n",
    "    train_set = shuffled_values[:split_index]\n",
    "    test_set = shuffled_values[split_index:]\n",
    "    return torch.tensor(train_set), torch.tensor(test_set)\n",
    "\n",
    "\n",
    "def evaluate_train_test_dataset_tree(train_dataset, test_dataset, max_cart_depth, gini_entropy_function, min_obs_per_leaf=2):\n",
    "    tree = train_cart(train_dataset, max_cart_depth=max_cart_depth, gini_entropy_function=gini_entropy_function,\n",
    "                      min_obs_per_leaf=min_obs_per_leaf)\n",
    "    overall_accuracy = test_cart(tree, test_dataset)\n",
    "    f1_scores = tree.get_f1_scores_per_class()\n",
    "    print(f\"Overall accuracy: {overall_accuracy}\")\n",
    "    print(f\"F1 scores: {f1_scores}\")\n",
    "    return overall_accuracy, f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = [split_dataset() for x in range(10)]\n",
    "results_gini = {}\n",
    "results_entropy = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n",
      "Overall accuracy: 93.33333333333333\n",
      "F1 scores: {1: tensor(0.9964), 2: tensor(0.8882), 3: tensor(0.8746), 4: tensor(0.9833)}\n",
      "Run 2\n",
      "Overall accuracy: 94.33333333333334\n",
      "F1 scores: {1: tensor(0.9937), 2: tensor(0.9048), 3: tensor(0.8881), 4: tensor(0.9803)}\n",
      "Run 3\n",
      "Overall accuracy: 95.33333333333334\n",
      "F1 scores: {1: tensor(0.9968), 2: tensor(0.9193), 3: tensor(0.9004), 4: tensor(0.9932)}\n",
      "Run 4\n",
      "Overall accuracy: 95.83333333333334\n",
      "F1 scores: {1: tensor(0.9904), 2: tensor(0.9333), 3: tensor(0.9111), 4: tensor(0.9934)}\n",
      "Run 5\n",
      "Overall accuracy: 94.66666666666667\n",
      "F1 scores: {1: tensor(0.9932), 2: tensor(0.9049), 3: tensor(0.8949), 4: tensor(0.9935)}\n",
      "Run 6\n",
      "Overall accuracy: 94.5\n",
      "F1 scores: {1: tensor(0.9844), 2: tensor(0.9147), 3: tensor(0.8803), 4: tensor(0.9847)}\n",
      "Run 7\n",
      "Overall accuracy: 94.83333333333334\n",
      "F1 scores: {1: tensor(0.9862), 2: tensor(0.9122), 3: tensor(0.9079), 4: tensor(0.9900)}\n",
      "Run 8\n",
      "Overall accuracy: 95.0\n",
      "F1 scores: {1: tensor(0.9935), 2: tensor(0.9156), 3: tensor(0.8990), 4: tensor(0.9898)}\n",
      "Run 9\n",
      "Overall accuracy: 91.33333333333333\n",
      "F1 scores: {1: tensor(0.9871), 2: tensor(0.8561), 3: tensor(0.8492), 4: tensor(0.9643)}\n",
      "Run 10\n",
      "Overall accuracy: 95.83333333333334\n",
      "F1 scores: {1: tensor(0.9966), 2: tensor(0.9195), 3: tensor(0.9221), 4: tensor(0.9967)}\n",
      "CPU times: total: 28.6 s\n",
      "Wall time: 2.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for index, partition in enumerate(partitions):\n",
    "    print(\"Run\",index + 1)\n",
    "    results_gini[f\"{index}, 3\"] = evaluate_train_test_dataset_tree(partition[0], partition[1], max_cart_depth=3, gini_entropy_function=\"GINI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n",
      "Overall accuracy: 93.5\n",
      "F1 scores: {1: tensor(0.9964), 2: tensor(0.8917), 3: tensor(0.8774), 4: tensor(0.9833)}\n",
      "Run 2\n",
      "Overall accuracy: 94.83333333333334\n",
      "F1 scores: {1: tensor(0.9937), 2: tensor(0.9123), 3: tensor(0.9017), 4: tensor(0.9803)}\n",
      "Run 3\n",
      "Overall accuracy: 96.33333333333334\n",
      "F1 scores: {1: tensor(0.9968), 2: tensor(0.9367), 3: tensor(0.9242), 4: tensor(0.9932)}\n",
      "Run 4\n",
      "Overall accuracy: 96.83333333333334\n",
      "F1 scores: {1: tensor(0.9904), 2: tensor(0.9515), 3: tensor(0.9348), 4: tensor(0.9934)}\n",
      "Run 5\n",
      "Overall accuracy: 96.0\n",
      "F1 scores: {1: tensor(0.9897), 2: tensor(0.9342), 3: tensor(0.9257), 4: tensor(0.9903)}\n",
      "Run 6\n",
      "Overall accuracy: 95.33333333333334\n",
      "F1 scores: {1: tensor(0.9844), 2: tensor(0.9301), 3: tensor(0.9023), 4: tensor(0.9847)}\n",
      "Run 7\n",
      "Overall accuracy: 95.66666666666667\n",
      "F1 scores: {1: tensor(0.9862), 2: tensor(0.9311), 3: tensor(0.9216), 4: tensor(0.9900)}\n",
      "Run 8\n",
      "Overall accuracy: 96.0\n",
      "F1 scores: {1: tensor(0.9935), 2: tensor(0.9359), 3: tensor(0.9187), 4: tensor(0.9898)}\n",
      "Run 9\n",
      "Overall accuracy: 93.5\n",
      "F1 scores: {1: tensor(0.9776), 2: tensor(0.9109), 3: tensor(0.8831), 4: tensor(0.9710)}\n",
      "Run 10\n",
      "Overall accuracy: 95.33333333333334\n",
      "F1 scores: {1: tensor(0.9965), 2: tensor(0.9059), 3: tensor(0.9154), 4: tensor(0.9967)}\n",
      "CPU times: total: 33 s\n",
      "Wall time: 2.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for index, partition in enumerate(partitions):\n",
    "    print(\"Run\",index+1)\n",
    "    results_gini[f\"{index}, 4\"] = evaluate_train_test_dataset_tree(partition[0], partition[1], max_cart_depth=4, gini_entropy_function=\"GINI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n",
      "Overall accuracy: 93.0\n",
      "F1 scores: {1: tensor(0.9854), 2: tensor(0.8982), 3: tensor(0.8601), 4: tensor(0.9833)}\n",
      "Run 2\n",
      "Overall accuracy: 94.33333333333334\n",
      "F1 scores: {1: tensor(0.9937), 2: tensor(0.9048), 3: tensor(0.8881), 4: tensor(0.9803)}\n",
      "Run 3\n",
      "Overall accuracy: 95.0\n",
      "F1 scores: {1: tensor(0.9903), 2: tensor(0.9206), 3: tensor(0.8936), 4: tensor(0.9932)}\n",
      "Run 4\n",
      "Overall accuracy: 95.0\n",
      "F1 scores: {1: tensor(0.9810), 2: tensor(0.9288), 3: tensor(0.8880), 4: tensor(0.9934)}\n",
      "Run 5\n",
      "Overall accuracy: 94.66666666666667\n",
      "F1 scores: {1: tensor(0.9932), 2: tensor(0.9049), 3: tensor(0.8949), 4: tensor(0.9935)}\n",
      "Run 6\n",
      "Overall accuracy: 94.5\n",
      "F1 scores: {1: tensor(0.9844), 2: tensor(0.9147), 3: tensor(0.8803), 4: tensor(0.9847)}\n",
      "Run 7\n",
      "Overall accuracy: 94.5\n",
      "F1 scores: {1: tensor(0.9792), 2: tensor(0.9122), 3: tensor(0.9022), 4: tensor(0.9900)}\n",
      "Run 8\n",
      "Overall accuracy: 94.16666666666667\n",
      "F1 scores: {1: tensor(0.9779), 2: tensor(0.9156), 3: tensor(0.8786), 4: tensor(0.9898)}\n",
      "Run 9\n",
      "Overall accuracy: 94.0\n",
      "F1 scores: {1: tensor(0.9872), 2: tensor(0.8977), 3: tensor(0.8903), 4: tensor(0.9891)}\n",
      "Run 10\n",
      "Overall accuracy: 95.0\n",
      "F1 scores: {1: tensor(0.9797), 2: tensor(0.9195), 3: tensor(0.9043), 4: tensor(0.9967)}\n",
      "CPU times: total: 32.4 s\n",
      "Wall time: 2.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for index, partition in enumerate(partitions):\n",
    "    print(\"Run\",index+1)\n",
    "    results_entropy[f\"{index}, 3\"] = evaluate_train_test_dataset_tree(partition[0], partition[1], max_cart_depth=3, gini_entropy_function=\"ENTROPY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n",
      "Overall accuracy: 94.33333333333334\n",
      "F1 scores: {1: tensor(0.9856), 2: tensor(0.9177), 3: tensor(0.8925), 4: tensor(0.9833)}\n",
      "Run 2\n",
      "Overall accuracy: 95.0\n",
      "F1 scores: {1: tensor(0.9937), 2: tensor(0.9155), 3: tensor(0.9054), 4: tensor(0.9803)}\n",
      "Run 3\n",
      "Overall accuracy: 95.0\n",
      "F1 scores: {1: tensor(0.9904), 2: tensor(0.9206), 3: tensor(0.8929), 4: tensor(0.9932)}\n",
      "Run 4\n",
      "Overall accuracy: 95.0\n",
      "F1 scores: {1: tensor(0.9870), 2: tensor(0.9122), 3: tensor(0.9010), 4: tensor(0.9967)}\n",
      "Run 5\n",
      "Overall accuracy: 95.66666666666667\n",
      "F1 scores: {1: tensor(0.9932), 2: tensor(0.9220), 3: tensor(0.9180), 4: tensor(0.9935)}\n",
      "Run 6\n",
      "Overall accuracy: 93.83333333333333\n",
      "F1 scores: {1: tensor(0.9844), 2: tensor(0.8938), 3: tensor(0.8746), 4: tensor(0.9847)}\n",
      "Run 7\n",
      "Overall accuracy: 95.33333333333334\n",
      "F1 scores: {1: tensor(0.9796), 2: tensor(0.9290), 3: tensor(0.9122), 4: tensor(0.9933)}\n",
      "Run 8\n",
      "Overall accuracy: 93.0\n",
      "F1 scores: {1: tensor(0.9776), 2: tensor(0.8858), 3: tensor(0.8647), 4: tensor(0.9898)}\n",
      "Run 9\n",
      "Overall accuracy: 93.33333333333333\n",
      "F1 scores: {1: tensor(0.9809), 2: tensor(0.8874), 3: tensor(0.8805), 4: tensor(0.9891)}\n",
      "Run 10\n",
      "Overall accuracy: 96.16666666666667\n",
      "F1 scores: {1: tensor(0.9932), 2: tensor(0.9306), 3: tensor(0.9274), 4: tensor(0.9967)}\n",
      "CPU times: total: 42.4 s\n",
      "Wall time: 3.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for index, partition in enumerate(partitions):\n",
    "    print(\"Run\",index+1)\n",
    "    results_entropy[f\"{index}, 4\"] = evaluate_train_test_dataset_tree(partition[0], partition[1], max_cart_depth=4, gini_entropy_function=\"ENTROPY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (15 puntos extra) Utilizando Optuna o weights and biases, optimice la profundidad máxima de los nodos. Muestre los gráficos de la optimización realizada con la herramienta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    dataset = read_dataset(csv_name=\"wifi_localization.txt\")\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 8)\n",
    "    tree = train_cart(dataset, max_cart_depth=max_depth, gini_entropy_function=\"ENTROPY\")\n",
    "    accuracy = test_cart(tree, dataset)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-06 23:12:00,016] A new study created in memory with name: no-name-f862befc-14fa-4bd9-ac8e-b5d34c331eb9\n",
      "[I 2024-04-06 23:12:00,485] Trial 0 finished with value: 97.15 and parameters: {'max_depth': 6}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:00,818] Trial 1 finished with value: 94.85 and parameters: {'max_depth': 3}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:01,079] Trial 2 finished with value: 93.8 and parameters: {'max_depth': 2}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:01,430] Trial 3 finished with value: 94.85 and parameters: {'max_depth': 3}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:01,865] Trial 4 finished with value: 95.89999999999999 and parameters: {'max_depth': 4}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:02,358] Trial 5 finished with value: 97.15 and parameters: {'max_depth': 8}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:02,605] Trial 6 finished with value: 93.8 and parameters: {'max_depth': 2}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:03,098] Trial 7 finished with value: 97.15 and parameters: {'max_depth': 6}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:03,263] Trial 8 finished with value: 50.0 and parameters: {'max_depth': 1}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:03,607] Trial 9 finished with value: 94.85 and parameters: {'max_depth': 3}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:04,103] Trial 10 finished with value: 97.15 and parameters: {'max_depth': 6}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:04,580] Trial 11 finished with value: 97.15 and parameters: {'max_depth': 8}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:05,053] Trial 12 finished with value: 97.15 and parameters: {'max_depth': 8}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:05,520] Trial 13 finished with value: 97.15 and parameters: {'max_depth': 6}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:06,022] Trial 14 finished with value: 97.15 and parameters: {'max_depth': 7}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:06,459] Trial 15 finished with value: 96.89999999999999 and parameters: {'max_depth': 5}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:06,968] Trial 16 finished with value: 97.15 and parameters: {'max_depth': 7}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:07,460] Trial 17 finished with value: 97.15 and parameters: {'max_depth': 7}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:07,916] Trial 18 finished with value: 96.89999999999999 and parameters: {'max_depth': 5}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:08,456] Trial 19 finished with value: 97.15 and parameters: {'max_depth': 8}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:08,963] Trial 20 finished with value: 97.15 and parameters: {'max_depth': 7}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:09,456] Trial 21 finished with value: 97.15 and parameters: {'max_depth': 6}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:09,932] Trial 22 finished with value: 96.89999999999999 and parameters: {'max_depth': 5}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:10,395] Trial 23 finished with value: 97.15 and parameters: {'max_depth': 6}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:10,810] Trial 24 finished with value: 95.89999999999999 and parameters: {'max_depth': 4}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:11,302] Trial 25 finished with value: 97.15 and parameters: {'max_depth': 7}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:11,822] Trial 26 finished with value: 97.15 and parameters: {'max_depth': 8}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:12,342] Trial 27 finished with value: 97.15 and parameters: {'max_depth': 6}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:12,850] Trial 28 finished with value: 96.89999999999999 and parameters: {'max_depth': 5}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:13,371] Trial 29 finished with value: 95.89999999999999 and parameters: {'max_depth': 4}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:13,900] Trial 30 finished with value: 97.15 and parameters: {'max_depth': 7}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:14,418] Trial 31 finished with value: 97.15 and parameters: {'max_depth': 6}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:14,942] Trial 32 finished with value: 97.15 and parameters: {'max_depth': 6}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:15,425] Trial 33 finished with value: 96.89999999999999 and parameters: {'max_depth': 5}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:15,912] Trial 34 finished with value: 97.15 and parameters: {'max_depth': 6}. Best is trial 0 with value: 97.15.\n",
      "[I 2024-04-06 23:12:16,384] Trial 35 finished with value: 97.15 and parameters: {'max_depth': 8}. Best is trial 0 with value: 97.15.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: 6\n",
      "Best accuracy: 97.15%\n",
      "CPU times: total: 3min 16s\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=36)\n",
    "\n",
    "lowest_depth = None\n",
    "for trial in study.best_trials:\n",
    "    depth = trial.params['max_depth']\n",
    "    lowest_depth = depth if lowest_depth is None or depth < lowest_depth else lowest_depth\n",
    "\n",
    "print(f\"Best parameters: {lowest_depth}\")\n",
    "print(f\"Best accuracy: {study.best_value}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "5020b9ee0a8929ac5eb1b3fdb8306c158218ab19e75e568837c0f095b545b9d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
